{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/ik/Desktop/zepeto')\n",
    "from a_python.rigging_class.rig_hier_maya_torch import *\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import pytorch3d\n",
    "from a_python.utils.rot import _rot_base\n",
    "\n",
    "from pytorch3d.io import load_objs_as_meshes, save_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVOrthographicCameras,\n",
    "    OpenGLOrthographicCameras,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    SoftPhongShader\n",
    ")\n",
    "from pytorch3d.renderer.mesh import rasterize_meshes\n",
    "from pytorch3d.renderer.mesh.rasterize_meshes import rasterize_meshes_python\n",
    "\n",
    "from pytorch3d.renderer.blending import BlendParams\n",
    "from pytorch3d.renderer.blending import sigmoid_alpha_blend, _sigmoid_alpha\n",
    "\n",
    "\n",
    "\n",
    "def img_upperbody_maya_joints(keypts):\n",
    "    '''maya관절 길이비율대로 상체 조인트 추가 : chest upper, chest, spine'''\n",
    "    img_hip = (keypts[9,:2] + keypts[12,:2])/2\n",
    "    img_neck = (keypts[2,:2] + keypts[5,:2])/2\n",
    "    img_spine = (img_neck - img_hip) * len_joint_upper[-1] + img_hip\n",
    "    img_chest = (img_neck - img_hip) * (len_joint_upper[-1] + len_joint_upper[-2]) + img_hip\n",
    "    img_chsetupper = (img_neck - img_hip) * (len_joint_upper[-1] + len_joint_upper[-2] + len_joint_upper[-3])  + img_hip\n",
    "    return img_hip, img_neck, img_spine, img_chest, img_chsetupper\n",
    "\n",
    "def fitting_based_on_upperbody_joints(body, img_spine, img_hip, img_neck, _vertices):\n",
    "    '''img_upperbody_maya_joints 함수 결과 사용해서 메쉬 스케일링으로 상체 조인트위치 맞춤'''\n",
    "    # hip to spine 거리\n",
    "    _len_spine_maya = np.linalg.norm(body.head - body.childs[0].head) \n",
    "    _len_spine_img = np.linalg.norm(img_spine - img_hip)\n",
    "\n",
    "    # 상체 조인트 맞춤\n",
    "    _spine_trans = -(_len_spine_maya - _len_spine_img) * \\\n",
    "        ((-body.head + body.childs[0].head) / np.linalg.norm(body.head - body.childs[0].head))\n",
    "    body.childs[0].trans_head(_spine_trans, _vertices)\n",
    "    _scale_spine_hier = (np.linalg.norm(img_hip - img_neck) * len_joint_upper[-1]) / \\\n",
    "        (np.linalg.norm(body.childs[0].head - body.childs[0].childs[0].head))\n",
    "    body.childs[0].scale_iso_hier(_scale_spine_hier, _vertices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Load / Leg Length Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_img = 11\n",
    "folder = idx_img\n",
    "path_img = \"/Users/ik/Downloads/test_set_new/{}/test_resized.png\".format(folder)\n",
    "path_json = \"/Users/ik/Downloads/test_set_new/{}/test_resized_keypoints.json\".format(folder)\n",
    "path_dp = \"/Users/ik/Downloads/test_set_new/{}/dp_dump.pkl\".format(folder)\n",
    "\n",
    "with open(path_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "with open(path_dp, 'rb') as f:\n",
    "    [img_seg, img_v, img_u,_] = pkl.load(f)\n",
    "h,w = img_seg.shape\n",
    "keypts = torch.tensor(data['people'][0]['pose_keypoints_2d']).reshape([-1,3])\n",
    "img = cv.imread(path_img)\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "body = rig_class(65)\n",
    "\n",
    "\n",
    "h, w = img_seg.shape\n",
    "x = np.linspace(0, w-1, w)\n",
    "y = np.linspace(0, h-1, h)\n",
    "xx, yy = np.meshgrid(x,y, indexing='xy')\n",
    "x = torch.linspace(0, w-1, w)\n",
    "y = torch.linspace(0, h-1, h)\n",
    "\n",
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# plt.imshow(img_seg)\n",
    "_vertices = copy.deepcopy(vertices)\n",
    "# 가랑이 찾고, 하체 다리길이 맞추기용 키포인트 길이 구하기 : 가랑이부분 허벅지 두께, 무릎, 발목 폭\n",
    "crotch = body.scaling_joint_len(_vertices, keypts, img_seg)\n",
    "len_maya_thigh_crotch, len_maya_knee, len_maya_ankle = body.len_leg_seg(_vertices)\n",
    "\n",
    "# 길이비율로 상체 오픈포즈에 마야 조인트 추가, 메쉬 맞추기\n",
    "img_hip, img_neck, img_spine, img_chest, img_chsetupper = img_upperbody_maya_joints(keypts)\n",
    "fitting_based_on_upperbody_joints(body, img_spine, img_hip, img_neck, _vertices)\n",
    "\n",
    "# base mesh for face index\n",
    "mesh = load_objs_as_meshes(['/Users/ik/Desktop/zepeto/blender/wip_find_weight.obj'], device)\n",
    "\n",
    "img_size = h\n",
    "\n",
    "mesh._verts_list[0] = (_vertices - _vertices[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2\n",
    "\n",
    "tmp_scale = torch.tensor([1.0])\n",
    "tmp_scale.requires_grad = True\n",
    "mesh._verts_list[0] *= tmp_scale\n",
    "pre =time.time()\n",
    "fragments = rasterize_meshes(mesh, image_size = img_size, blur_radius=0, faces_per_pixel=8,\\\n",
    "     perspective_correct= False, )\n",
    "\n",
    "print(time.time() - pre)\n",
    "plt.figure('rasterize')\n",
    "plt.imshow(np.asarray(torch.sum(fragments[1][0], dim=2).detach()/8+1))\n",
    "plt.figure('img')\n",
    "plt.imshow(img_seg)\n",
    "\n",
    "body_init = copy.deepcopy(body)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesh 3D Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_joint = []\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "_vertices_draw = np.asarray(copy.deepcopy(_vertices.detach()))\n",
    "\n",
    "# _vertices_draw = np.asarray(copy.deepcopy(_vertices.detach()))\n",
    "pcd.points = o3d.utility.Vector3dVector(_vertices_draw)\n",
    "_color = np.ones([9067,3])*np.array([0.5,0.3,1])\n",
    "pcd.colors = o3d.utility.Vector3dVector(_color)\n",
    "# _pcd = o3d.geometry.PointCloud()\n",
    "# _pcd.points = o3d.utility.Vector3dVector(vertices)\n",
    "# _list_joint.append(_pcd)\n",
    "_list_joint.append(pcd)\n",
    "body.draw_joint(_list_joint)\n",
    "o3d.visualization.draw_geometries(_list_joint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask = (img_seg!=0).astype(int)\n",
    "# 허벅지 부분 이미지\n",
    "thr_surplus = 3\n",
    "mask_leg = (img_seg == 10) + (img_seg == 9)\n",
    "bbox = np.array([[xx[mask_leg].min()-thr_surplus , yy[mask_leg].min()-thr_surplus],[xx[mask_leg].max()+thr_surplus , yy[mask_leg].max()+thr_surplus]]).astype(int)\n",
    "_ul, _lr = torch.max(torch.abs(torch.from_numpy(bbox) - crotch), dim=1).values\n",
    "bbox = np.array([[crotch[0] - _ul, crotch[1] - _ul],[crotch[0] + _lr, crotch[1] + _lr]]).astype(int)\n",
    "crotch_subimg = torch.tensor([_ul, _ul])\n",
    "\n",
    "\n",
    "# 변형된 메쉬\n",
    "_vertices_opt = copy.deepcopy(_vertices)\n",
    "opt_x = torch.nn.Parameter(torch.tensor([0.,0.,1.,1.,1.,1., 0., 0.]))\n",
    "\n",
    "'''opt_x : uppperlegtrans_x, uppperlegtrans_y, upperlegscale, uppertwistscale, scale_iso_upper, scale_iso_upper_twist'''\n",
    "body.childs[1].childs[0].trans_head(torch.tensor([opt_x[0],opt_x[1],0]), _vertices_opt)\n",
    "body.childs[1].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "body.childs[1].childs[0].scale_z(opt_x[2], _vertices_opt)\n",
    "body.childs[1].childs[0].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "body.childs[1].childs[0].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "# mesh._verts_packed = (_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2\n",
    "\n",
    "# 허벅지 crop에 맞게 cropped rasterize\n",
    "mesh_crotch = (_vertices_opt[3106,:] - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) \\\n",
    "    - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2\n",
    "mesh_crotch = mesh_crotch.detach()\n",
    "tmp = (_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - \\\n",
    "    torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2\n",
    "_mesh_scale = img_size / (_ul + _lr)\n",
    "_move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]/(_ul + _lr)\n",
    "_move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]/(_ul + _lr)\n",
    "\n",
    "mesh._verts_packed = ((_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - \\\n",
    "    torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2) * _mesh_scale - torch.tensor([_move_x, _move_y, 0])\n",
    "fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "\n",
    "if True:\n",
    "    plt.figure('subimg')\n",
    "    plt.imshow(img_seg[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]])\n",
    "    plt.scatter(22,21)\n",
    "    plt.figure('rasterize')\n",
    "    plt.imshow(np.asarray(torch.sum(fragments[1][0], dim=2).detach()/8+1))\n",
    "    plt.scatter(22,21)\n",
    "\n",
    "# loss\n",
    "## Trial 1 rasterization 결과에 1 넣어서\n",
    "mask = fragments[1][0].sum(dim=2) > 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1 : Using Shader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_params = BlendParams()\n",
    "    # sigma: float = 1e-4\n",
    "    # gamma: float = 1e-4\n",
    "\n",
    "\n",
    "colors = torch.ones_like(fragments[2])\n",
    "# blend_params = kwargs.get(\"blend_params\", self.blend_params)\n",
    "# images = sigmoid_alpha_blend(colors, fragments, blend_params)\n",
    "\n",
    "N, H, W, K = fragments[0].shape\n",
    "pixel_colors = torch.ones((N, H, W, 4), dtype=colors.dtype, device=colors.device)\n",
    "pixel_colors[..., :3] = colors[..., 0, :]\n",
    "alpha = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "pixel_colors[..., 3] = alpha\n",
    "\n",
    "if True:\n",
    "    tmp = pixel_colors[0].detach()\n",
    "    tmp_ = tmp[:,:,3]\n",
    "    print(tmp.shape)\n",
    "    plt.figure('3')\n",
    "    plt.imshow(tmp[:,:,3])\n",
    "    # plt.figure('0')\n",
    "    # plt.imshow(tmp[:,:,0])\n",
    "    # plt.figure('1')\n",
    "    # plt.imshow(tmp[:,:,1])\n",
    "    # plt.figure('2')\n",
    "    # plt.imshow(tmp[:,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _rot_base(axis, angle):\n",
    "#     if axis == 'x':\n",
    "#         return torch.tensor(\n",
    "#             [\n",
    "#                 [1, 0, 0],\n",
    "#                 [0, torch.cos(angle), -torch.sin(angle)],\n",
    "#                 [0, torch.sin(angle), torch.cos(angle)]\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#     if axis == 'y':\n",
    "#         return torch.tensor(\n",
    "#             [\n",
    "#                 [torch.cos(angle), 0, torch.sin(angle)],\n",
    "#                 [0, 1, 0],\n",
    "#                 [-torch.sin(angle), 0, torch.cos(angle)]\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#     if axis == 'z':\n",
    "#         return torch.tensor(\n",
    "#             [\n",
    "#                 [torch.cos(angle), -torch.sin(angle), 0],\n",
    "#                 [torch.sin(angle), torch.cos(angle), 0],\n",
    "#                 [0, 0, 1],\n",
    "#             ]\n",
    "#         )\n",
    "def _rot_base(axis, angle):\n",
    "    zero = torch.zeros_like(angle)\n",
    "    one = torch.ones_like(angle)\n",
    "    if axis == 'x':\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                torch.stack([one, zero, zero]),\n",
    "                torch.stack([0, torch.cos(angle), -torch.sin(angle)]),\n",
    "                torch.stack([0, torch.sin(angle), torch.cos(angle)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if axis == 'y':\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                torch.stack([torch.cos(angle), zero, torch.sin(angle)]),\n",
    "                torch.stack([zero, one, zero]),\n",
    "                torch.stack([-torch.sin(angle), zero, torch.cos(angle)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if axis == 'z':\n",
    "        return torch.stack(\n",
    "            [\n",
    "                torch.stack([torch.cos(angle), -torch.sin(angle), zero]),\n",
    "                torch.stack([torch.sin(angle), torch.cos(angle), zero]),\n",
    "                torch.stack([zero, zero, one]),\n",
    "            ]\n",
    "        )\n",
    "def deform_mesh(opt_x, body, _vertices_opt):\n",
    "    body.childs[1].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "    \n",
    "    body.childs[1].childs[1].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "\n",
    "    body.childs[1].childs[0].rot(_rot_base('z', opt_x[0]), _vertices_opt, inplace=False)\n",
    "    # body.childs[1].childs[1].rot(_rot_base('z', opt_x[1]), _vertices_opt)\n",
    "\n",
    "\n",
    "def loss_dr(img_target, opt_x, body, _vertices, crotch, img_size, _ul, _lr, mesh_crotch, img_weight = None):\n",
    "    # mesh change\n",
    "    _vertices_opt = copy.deepcopy(_vertices)\n",
    "    deform_mesh(opt_x, body, _vertices_opt)\n",
    "\n",
    "    # 허벅지 crop에 맞게 cropped rasterize\n",
    "    _mesh_scale = img_size / (_ul + _lr)\n",
    "    # _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]/(_ul + _lr)\n",
    "    # _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]/(_ul + _lr)\n",
    "    _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]\n",
    "    _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]\n",
    "\n",
    "    mesh._verts_packed = ((_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2) * _mesh_scale - torch.tensor([_move_x, _move_y, 0])\n",
    "    fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "    res = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "    if img_weight is None:\n",
    "        loss = (torch.abs(img_target - res)).sum()\n",
    "    else:\n",
    "        loss = (img_weight * torch.abs(img_target - res)).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_dr(img_target, opt_x, body, _vertices, crotch, img_size, _ul, _lr, mesh_crotch):\n",
    "#     # mesh change\n",
    "#     _vertices_opt = copy.deepcopy(_vertices)\n",
    "#     body.childs[1].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "#     body.childs[1].childs[0].scale_z(opt_x[2], _vertices_opt)\n",
    "#     body.childs[1].childs[0].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "#     body.childs[1].childs[0].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "#     body.childs[1].childs[1].scale_y(opt_x[2], _vertices_opt)\n",
    "#     body.childs[1].childs[1].scale_z(opt_x[2], _vertices_opt)\n",
    "#     body.childs[1].childs[1].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "#     body.childs[1].childs[1].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "\n",
    "#     # 허벅지 crop에 맞게 cropped rasterize\n",
    "#     _mesh_scale = img_size / (_ul + _lr)\n",
    "#     _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]/(_ul + _lr)\n",
    "#     _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]/(_ul + _lr)\n",
    "\n",
    "#     mesh._verts_packed = ((_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2) * _mesh_scale - torch.tensor([_move_x, _move_y, 0])\n",
    "#     fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "#     res = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "#     loss = (torch.abs(img_target - res)).sum()\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic settings\n",
    "## parameter\n",
    "alpha = 10\n",
    "opt_x = torch.nn.Parameter(torch.tensor([0.,0.,1.,1.,1.,1., 0., 0.]))\n",
    "'''opt_x : uppperlegtrans_x, uppperlegtrans_y, upperlegscale, uppertwistscale, scale_iso_upper, scale_iso_upper_twist, move_x, move_y'''\n",
    "## rasterize 관련\n",
    "blend_params = BlendParams()\n",
    "colors = torch.ones_like(fragments[2])\n",
    "## GT : 허벅지 부분 이미지\n",
    "img_mask = (img_seg!=0).astype(int)\n",
    "thr_surplus = 3\n",
    "mask_leg = (img_seg == 10) + (img_seg == 9)\n",
    "bbox = np.array([[xx[mask_leg].min()-thr_surplus , yy[mask_leg].min()-thr_surplus],[xx[mask_leg].max()+thr_surplus , yy[mask_leg].max()+thr_surplus]]).astype(int)\n",
    "_ul, _lr = torch.max(torch.abs(torch.from_numpy(bbox) - crotch), dim=1).values\n",
    "bbox = np.array([[crotch[0] - _ul, crotch[1] - _ul],[crotch[0] + _lr, crotch[1] + _lr]]).astype(int)\n",
    "crotch_subimg = torch.tensor([_ul, _ul])\n",
    "img_target = torch.from_numpy((img_seg[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]]>0).astype(int))\n",
    "tmp_img_weight = (img_seg[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]]>0).astype(int)\n",
    "img_weight = torch.from_numpy((tmp_img_weight - 1) * (-alpha) + tmp_img_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "opt_x = torch.nn.Parameter(torch.tensor([0.05,0.05,1.,1.,1.,1., 0., 0.]))\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params' : opt_x}], lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam([{'params' : opt_x}], lr=0.01)\n",
    "\n",
    "\n",
    "loss_all = []\n",
    "result_opt_x = []\n",
    "n_iter = 10\n",
    "\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_dr(img_target, opt_x, body, _vertices, crotch, img_size, _ul, _lr, mesh_crotch, img_weight)\n",
    "\n",
    "    print('loss:', loss)\n",
    "    loss_all.append(loss.detach())\n",
    "    # print('opt_x:', opt_x)\n",
    "    # _result_opt_x = copy.deepcopy(opt_x)\n",
    "    # result_opt_x.append(np.asarray(_result_opt_x.detach()))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vertices_opt = copy.deepcopy(_vertices)\n",
    "deform_mesh(opt_x, body, _vertices_opt)\n",
    "\n",
    "\n",
    "# 허벅지 crop에 맞게 cropped rasterize\n",
    "_mesh_scale = img_size / (_ul + _lr)\n",
    "_move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]\n",
    "_move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]\n",
    "\n",
    "mesh._verts_packed = ((_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2) * _mesh_scale - torch.tensor([_move_x, _move_y, 0])\n",
    "fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "res = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_x)\n",
    "plt.figure('GT')\n",
    "plt.imshow(img_target)\n",
    "plt.figure('optimization_result')\n",
    "plt.imshow(np.asarray(res[0].detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad function check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deform_mesh(opt_x, body, _vertices_opt):\n",
    "    body.childs[1].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "    \n",
    "    body.childs[1].childs[1].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "\n",
    "    body.childs[1].childs[0].rot(_rot_base('z', opt_x[0]), _vertices_opt)\n",
    "    body.childs[1].childs[1].rot(_rot_base('z', opt_x[1]), _vertices_opt)\n",
    "\n",
    "\n",
    "def loss_dr(img_target, opt_x, body, _vertices, crotch, img_size, _ul, _lr, mesh_crotch, img_weight = None):\n",
    "    # mesh change\n",
    "    _vertices_opt = copy.deepcopy(_vertices)\n",
    "    deform_mesh(opt_x, body, _vertices_opt)\n",
    "\n",
    "    # 허벅지 crop에 맞게 cropped rasterize\n",
    "    _mesh_scale = img_size / (_ul + _lr)\n",
    "    # _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]/(_ul + _lr)\n",
    "    # _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]/(_ul + _lr)\n",
    "    _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]\n",
    "    _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]\n",
    "\n",
    "    mesh._verts_packed = ((_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2) * _mesh_scale - torch.tensor([_move_x, _move_y, 0])\n",
    "    fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "    res = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "    if img_weight is None:\n",
    "        loss = (torch.abs(img_target - res)).sum()\n",
    "    else:\n",
    "        loss = (img_weight * torch.abs(img_target - res)).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "opt_x = torch.nn.Parameter(torch.tensor([0.1,0.,.5,.5,1.,1., 0., 0.]))\n",
    "_vertices_opt = copy.deepcopy(_vertices)\n",
    "body.childs[1].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "body.childs[1].childs[0].scale_z(opt_x[2], _vertices_opt)\n",
    "body.childs[1].childs[0].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "body.childs[1].childs[0].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "\n",
    "body.childs[1].childs[1].scale_y(opt_x[2], _vertices_opt)\n",
    "body.childs[1].childs[1].scale_z(opt_x[2], _vertices_opt)\n",
    "body.childs[1].childs[1].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "body.childs[1].childs[1].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "\n",
    "body.childs[1].childs[0].rot(_rot_base('z', opt_x[0]), _vertices_opt, inplace=False)\n",
    "\n",
    "# 허벅지 crop에 맞게 cropped rasterize\n",
    "_mesh_scale = img_size / (_ul + _lr)\n",
    "_move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]/(_ul + _lr)\n",
    "_move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]/(_ul + _lr)\n",
    "\n",
    "mesh._verts_packed = ((_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2) * _mesh_scale - torch.tensor([_move_x, _move_y, 0])\n",
    "fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "res = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "plt.imshow(np.asarray(res[0].detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_joint = []\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "_vertices_draw = np.asarray(copy.deepcopy(_vertices_opt.detach()))\n",
    "\n",
    "# _vertices_draw = np.asarray(copy.deepcopy(_vertices.detach()))\n",
    "pcd.points = o3d.utility.Vector3dVector(_vertices_draw)\n",
    "_color = np.ones([9067,3])*np.array([0.5,0.3,1])\n",
    "pcd.colors = o3d.utility.Vector3dVector(_color)\n",
    "# _pcd = o3d.geometry.PointCloud()\n",
    "# _pcd.points = o3d.utility.Vector3dVector(vertices)\n",
    "# _list_joint.append(_pcd)\n",
    "_list_joint.append(pcd)\n",
    "body.draw_joint(_list_joint)\n",
    "o3d.visualization.draw_geometries(_list_joint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a_python.utils.rot import _rot_base\n",
    "# body.childs[1].childs[1].rot(_rot_base('x', np.pi/3), _vertices_opt)\n",
    "tmp = torch.as_tensor(_rot_base('x', math.pi/3)).type(torch.FloatTensor)\n",
    "tmp = tmp.type(torch.FloatTensor)\n",
    "print(tmp)\n",
    " \n",
    "body.childs[1].childs[1].rot(torch.as_tensor(_rot_base('x', math.pi/3)).type(torch.FloatTensor), _vertices_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(body.childs[1].childs[1].childs[0].name)\n",
    "print(body.childs[1].childs[0].childs[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그냥 meshrenderer 써도 될거같은데"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# camera = FoVOrthographicCameras(device=device, zfar=200, \\\n",
    "#     T = torch.tensor([[0,0,10]]), znear=-100) \n",
    "# lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "# sigma = 1e-5\n",
    "# raster_settings_soft = RasterizationSettings(\n",
    "#     image_size=_ul + _lr, \n",
    "#     blur_radius=np.log(1. / sigma - 1.)*sigma, \n",
    "#     faces_per_pixel=50, \n",
    "# )\n",
    "\n",
    "# renderer_silhouette = MeshRenderer(rasterizer=MeshRasterizer(\n",
    "#     cameras=camera, raster_settings=raster_settings_soft),\n",
    "#     shader=SoftSilhouetteShader())\n",
    "\n",
    "# pre =time.time()\n",
    "# _img_ = renderer_silhouette(mesh, cameras=camera, lights=lights)\n",
    "# print(time.time() - pre)\n",
    "# plt.imshow(np.asarray(_img_[0,:,:,3].detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vertices_opt = copy.deepcopy(_vertices)\n",
    "body.childs[1].childs[1].rot(_rot_base('z', opt_x[1]), _vertices_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_tmp = body.childs[1].childs[1]\n",
    "_weight_sum = torch.zeros([_vertices_opt.shape[0]])\n",
    "body_tmp.concat_weights(_weight_sum)\n",
    "\n",
    "\n",
    "_vertices_opt[:,:] -= body_tmp.head\n",
    "_vertices_opt[:,:] += _weight_sum[:,None] * ((_rot_base('z', opt_x[1]) @ _vertices.T).T - _vertices )\n",
    "_vertices_opt[:,:] += body_tmp.head\n",
    "# body_tmp.rot_update(_rot_base('z', opt_x[1]), body_tmp.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_vertices_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/ik/Desktop/zepeto')\n",
    "from a_python.rigging_class.rig_hier_maya_torch import *\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import pytorch3d\n",
    "from a_python.utils.rot import _rot_base\n",
    "\n",
    "from pytorch3d.io import load_objs_as_meshes, save_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVOrthographicCameras,\n",
    "    OpenGLOrthographicCameras,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    SoftPhongShader\n",
    ")\n",
    "from pytorch3d.renderer.mesh import rasterize_meshes\n",
    "from pytorch3d.renderer.mesh.rasterize_meshes import rasterize_meshes_python\n",
    "\n",
    "from pytorch3d.renderer.blending import BlendParams\n",
    "from pytorch3d.renderer.blending import sigmoid_alpha_blend, _sigmoid_alpha\n",
    "\n",
    "\n",
    "\n",
    "def img_upperbody_maya_joints(keypts):\n",
    "    '''maya관절 길이비율대로 상체 조인트 추가 : chest upper, chest, spine'''\n",
    "    img_hip = (keypts[9,:2] + keypts[12,:2])/2\n",
    "    img_neck = (keypts[2,:2] + keypts[5,:2])/2\n",
    "    img_spine = (img_neck - img_hip) * len_joint_upper[-1] + img_hip\n",
    "    img_chest = (img_neck - img_hip) * (len_joint_upper[-1] + len_joint_upper[-2]) + img_hip\n",
    "    img_chsetupper = (img_neck - img_hip) * (len_joint_upper[-1] + len_joint_upper[-2] + len_joint_upper[-3])  + img_hip\n",
    "    return img_hip, img_neck, img_spine, img_chest, img_chsetupper\n",
    "\n",
    "def fitting_based_on_upperbody_joints(body, img_spine, img_hip, img_neck, _vertices):\n",
    "    '''img_upperbody_maya_joints 함수 결과 사용해서 메쉬 스케일링으로 상체 조인트위치 맞춤'''\n",
    "    # hip to spine 거리\n",
    "    _len_spine_maya = np.linalg.norm(body.head - body.childs[0].head) \n",
    "    _len_spine_img = np.linalg.norm(img_spine - img_hip)\n",
    "\n",
    "    # 상체 조인트 맞춤\n",
    "    _spine_trans = -(_len_spine_maya - _len_spine_img) * \\\n",
    "        ((-body.head + body.childs[0].head) / np.linalg.norm(body.head - body.childs[0].head))\n",
    "    body.childs[0].trans_head(_spine_trans, _vertices)\n",
    "    _scale_spine_hier = (np.linalg.norm(img_hip - img_neck) * len_joint_upper[-1]) / \\\n",
    "        (np.linalg.norm(body.childs[0].head - body.childs[0].childs[0].head))\n",
    "    body.childs[0].scale_iso_hier(_scale_spine_hier, _vertices)\n",
    "\n",
    "idx_img = 11\n",
    "folder = idx_img\n",
    "path_img = \"/Users/ik/Downloads/test_set_new/{}/test_resized.png\".format(folder)\n",
    "path_json = \"/Users/ik/Downloads/test_set_new/{}/test_resized_keypoints.json\".format(folder)\n",
    "path_dp = \"/Users/ik/Downloads/test_set_new/{}/dp_dump.pkl\".format(folder)\n",
    "\n",
    "with open(path_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "with open(path_dp, 'rb') as f:\n",
    "    [img_seg, img_v, img_u,_] = pkl.load(f)\n",
    "h,w = img_seg.shape\n",
    "keypts = torch.tensor(data['people'][0]['pose_keypoints_2d']).reshape([-1,3])\n",
    "img = cv.imread(path_img)\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "body = rig_class(65)\n",
    "\n",
    "\n",
    "h, w = img_seg.shape\n",
    "x = np.linspace(0, w-1, w)\n",
    "y = np.linspace(0, h-1, h)\n",
    "xx, yy = np.meshgrid(x,y, indexing='xy')\n",
    "x = torch.linspace(0, w-1, w)\n",
    "y = torch.linspace(0, h-1, h)\n",
    "\n",
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# plt.imshow(img_seg)\n",
    "_vertices = copy.deepcopy(vertices)\n",
    "# 가랑이 찾고, 하체 다리길이 맞추기용 키포인트 길이 구하기 : 가랑이부분 허벅지 두께, 무릎, 발목 폭\n",
    "crotch = body.scaling_joint_len(_vertices, keypts, img_seg)\n",
    "len_maya_thigh_crotch, len_maya_knee, len_maya_ankle = body.len_leg_seg(_vertices)\n",
    "\n",
    "# 길이비율로 상체 오픈포즈에 마야 조인트 추가, 메쉬 맞추기\n",
    "img_hip, img_neck, img_spine, img_chest, img_chsetupper = img_upperbody_maya_joints(keypts)\n",
    "fitting_based_on_upperbody_joints(body, img_spine, img_hip, img_neck, _vertices)\n",
    "\n",
    "# base mesh for face index\n",
    "mesh = load_objs_as_meshes(['/Users/ik/Desktop/zepeto/blender/wip_find_weight.obj'], device)\n",
    "\n",
    "img_size = h\n",
    "\n",
    "mesh._verts_list[0] = (_vertices - _vertices[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2\n",
    "\n",
    "tmp_scale = torch.tensor([1.0])\n",
    "tmp_scale.requires_grad = True\n",
    "mesh._verts_list[0] *= tmp_scale\n",
    "pre =time.time()\n",
    "fragments = rasterize_meshes(mesh, image_size = img_size, blur_radius=0, faces_per_pixel=8,\\\n",
    "     perspective_correct= False, )\n",
    "\n",
    "print(time.time() - pre)\n",
    "plt.figure('rasterize')\n",
    "plt.imshow(np.asarray(torch.sum(fragments[1][0], dim=2).detach()/8+1))\n",
    "plt.figure('img')\n",
    "plt.imshow(img_seg)\n",
    "\n",
    "# body class 카피 - optimization에서 사용\n",
    "body_init = copy.deepcopy(body)\n",
    "\n",
    "\n",
    "\n",
    "img_mask = (img_seg!=0).astype(int)\n",
    "# 허벅지 부분 이미지\n",
    "thr_surplus = 3\n",
    "mask_leg = (img_seg == 10) + (img_seg == 9)\n",
    "bbox = np.array([[xx[mask_leg].min()-thr_surplus , yy[mask_leg].min()-thr_surplus],[xx[mask_leg].max()+thr_surplus , yy[mask_leg].max()+thr_surplus]]).astype(int)\n",
    "_ul, _lr = torch.max(torch.abs(torch.from_numpy(bbox) - crotch), dim=1).values\n",
    "bbox = np.array([[crotch[0] - _ul, crotch[1] - _ul],[crotch[0] + _lr, crotch[1] + _lr]]).astype(int)\n",
    "crotch_subimg = torch.tensor([_ul, _ul])\n",
    "\n",
    "\n",
    "# 변형된 메쉬\n",
    "_vertices_opt = copy.deepcopy(_vertices)\n",
    "opt_x = torch.nn.Parameter(torch.tensor([0.,0.,1.,1.,1.,1., 0., 0.]))\n",
    "'''opt_x : uppperlegtrans_x, uppperlegtrans_y, upperlegscale, uppertwistscale, scale_iso_upper, scale_iso_upper_twist'''\n",
    "body.childs[1].childs[0].trans_head(torch.tensor([opt_x[0],opt_x[1],0]), _vertices_opt)\n",
    "body.childs[1].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "body.childs[1].childs[0].scale_z(opt_x[2], _vertices_opt)\n",
    "body.childs[1].childs[0].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "body.childs[1].childs[0].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "# mesh._verts_packed = (_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2\n",
    "\n",
    "# 허벅지 crop에 맞게 cropped rasterize\n",
    "mesh_crotch = (_vertices_opt[3106,:] - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) \\\n",
    "    - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2\n",
    "mesh_crotch = mesh_crotch.detach()\n",
    "tmp = (_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - \\\n",
    "    torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2\n",
    "_mesh_scale = img_size / (_ul + _lr)\n",
    "_move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]/(_ul + _lr)\n",
    "_move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]/(_ul + _lr)\n",
    "\n",
    "mesh._verts_packed = ((_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - \\\n",
    "    torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2) * _mesh_scale - torch.tensor([_move_x, _move_y, 0])\n",
    "fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "\n",
    "if True:\n",
    "    plt.figure('subimg')\n",
    "    plt.imshow(img_seg[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]])\n",
    "    plt.scatter(22,21)\n",
    "    plt.figure('rasterize')\n",
    "    plt.imshow(np.asarray(torch.sum(fragments[1][0], dim=2).detach()/8+1))\n",
    "    plt.scatter(22,21)\n",
    "\n",
    "# loss\n",
    "## Trial 1 rasterization 결과에 1 넣어서\n",
    "mask = fragments[1][0].sum(dim=2) > 0\n",
    "\n",
    "blend_params = BlendParams()\n",
    "    # sigma: float = 1e-4\n",
    "    # gamma: float = 1e-4\n",
    "\n",
    "\n",
    "colors = torch.ones_like(fragments[2])\n",
    "# blend_params = kwargs.get(\"blend_params\", self.blend_params)\n",
    "# images = sigmoid_alpha_blend(colors, fragments, blend_params)\n",
    "\n",
    "N, H, W, K = fragments[0].shape\n",
    "pixel_colors = torch.ones((N, H, W, 4), dtype=colors.dtype, device=colors.device)\n",
    "pixel_colors[..., :3] = colors[..., 0, :]\n",
    "alpha = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "pixel_colors[..., 3] = alpha\n",
    "\n",
    "if True:\n",
    "    tmp = pixel_colors[0].detach()\n",
    "    tmp_ = tmp[:,:,3]\n",
    "    print(tmp.shape)\n",
    "    plt.figure('3')\n",
    "    plt.imshow(tmp[:,:,3])\n",
    "\n",
    "def _rot_base(axis, angle):\n",
    "    zero = torch.zeros_like(angle)\n",
    "    one = torch.ones_like(angle)\n",
    "    if axis == 'x':\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                torch.stack([one, zero, zero]),\n",
    "                torch.stack([0, torch.cos(angle), -torch.sin(angle)]),\n",
    "                torch.stack([0, torch.sin(angle), torch.cos(angle)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if axis == 'y':\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                torch.stack([torch.cos(angle), zero, torch.sin(angle)]),\n",
    "                torch.stack([zero, one, zero]),\n",
    "                torch.stack([-torch.sin(angle), zero, torch.cos(angle)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if axis == 'z':\n",
    "        return torch.stack(\n",
    "            [\n",
    "                torch.stack([torch.cos(angle), -torch.sin(angle), zero]),\n",
    "                torch.stack([torch.sin(angle), torch.cos(angle), zero]),\n",
    "                torch.stack([zero, zero, one]),\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deform_mesh(opt_x, body, _vertices_opt):\n",
    "    body.childs[1].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "    \n",
    "    body.childs[1].childs[1].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "\n",
    "    body.childs[1].childs[0].rot(_rot_base('z', opt_x[0]), _vertices_opt, inplace=False)\n",
    "    # body.childs[1].childs[1].rot(_rot_base('z', opt_x[1]), _vertices_opt)\n",
    "\n",
    "\n",
    "def loss_dr(img_target, opt_x, body, _vertices, crotch, img_size, _ul, _lr, mesh_crotch, img_weight = None):\n",
    "    # mesh change\n",
    "    _vertices_opt = copy.deepcopy(_vertices)\n",
    "    body_copy = copy.deepcopy(body)\n",
    "    deform_mesh(opt_x, body_copy, _vertices_opt)\n",
    "\n",
    "    # 허벅지 crop에 맞게 cropped rasterize\n",
    "    _mesh_scale = img_size / (_ul + _lr)\n",
    "    # _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]/(_ul + _lr)\n",
    "    # _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]/(_ul + _lr)\n",
    "    _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]\n",
    "    _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]\n",
    "\n",
    "    mesh._verts_packed = ((_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2) * _mesh_scale - torch.tensor([_move_x, _move_y, 0])\n",
    "    fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "    res = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "    if img_weight is None:\n",
    "        loss = (torch.abs(img_target - res)).sum()\n",
    "    else:\n",
    "        loss = (img_weight * torch.abs(img_target - res)).sum()\n",
    "    return loss\n",
    "    # basic settings\n",
    "## parameter\n",
    "alpha = 10\n",
    "opt_x = torch.nn.Parameter(torch.tensor([0.,0.,1.,1.,1.,1., 0., 0.]))\n",
    "'''opt_x : uppperlegtrans_x, uppperlegtrans_y, upperlegscale, uppertwistscale, scale_iso_upper, scale_iso_upper_twist, move_x, move_y'''\n",
    "## rasterize 관련\n",
    "blend_params = BlendParams()\n",
    "colors = torch.ones_like(fragments[2])\n",
    "## GT : 허벅지 부분 이미지\n",
    "img_mask = (img_seg!=0).astype(int)\n",
    "thr_surplus = 3\n",
    "mask_leg = (img_seg == 10) + (img_seg == 9)\n",
    "bbox = np.array([[xx[mask_leg].min()-thr_surplus , yy[mask_leg].min()-thr_surplus],[xx[mask_leg].max()+thr_surplus , yy[mask_leg].max()+thr_surplus]]).astype(int)\n",
    "_ul, _lr = torch.max(torch.abs(torch.from_numpy(bbox) - crotch), dim=1).values\n",
    "bbox = np.array([[crotch[0] - _ul, crotch[1] - _ul],[crotch[0] + _lr, crotch[1] + _lr]]).astype(int)\n",
    "crotch_subimg = torch.tensor([_ul, _ul])\n",
    "img_target = torch.from_numpy((img_seg[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]]>0).astype(int))\n",
    "tmp_img_weight = (img_seg[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]]>0).astype(int)\n",
    "img_weight = torch.from_numpy((tmp_img_weight - 1) * (-alpha) + tmp_img_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "opt_x = torch.nn.Parameter(torch.tensor([0.05,0.05,1.,1.,1.,1., 0., 0.]))\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params' : opt_x}], lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam([{'params' : opt_x}], lr=0.01)\n",
    "\n",
    "\n",
    "loss_all = []\n",
    "result_opt_x = []\n",
    "n_iter = 10\n",
    "\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_dr(img_target, opt_x, body_init, _vertices, crotch, img_size, _ul, _lr, mesh_crotch, img_weight)\n",
    "\n",
    "    print('loss:', loss)\n",
    "    loss_all.append(loss.detach())\n",
    "    # print('opt_x:', opt_x)\n",
    "    # _result_opt_x = copy.deepcopy(opt_x)\n",
    "    # result_opt_x.append(np.asarray(_result_opt_x.detach()))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(opt_x)\n",
    "print(body)\n",
    "print(body_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "opt_x = torch.nn.Parameter(torch.tensor([0.1,0.05,1.,1.,1.,1., 0., 0.]))\n",
    "\n",
    "_vertices_opt = copy.deepcopy(_vertices)\n",
    "body_copy = copy.deepcopy(body_init)\n",
    "deform_mesh(opt_x, body_copy, _vertices_opt)\n",
    "\n",
    "# 허벅지 crop에 맞게 cropped rasterize\n",
    "_mesh_scale = img_size / (_ul + _lr)\n",
    "# _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]/(_ul + _lr)\n",
    "# _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]/(_ul + _lr)\n",
    "_move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] + opt_x[6]\n",
    "_move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1] + opt_x[7]\n",
    "\n",
    "mesh._verts_packed = ((_vertices_opt - _vertices_opt[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) - torch.tensor([img_size/2, img_size/2, -img_size/2]))/256 * 2) * _mesh_scale - torch.tensor([_move_x, _move_y, 0])\n",
    "fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "res = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "# print(opt_x)\n",
    "plt.figure('GT')\n",
    "plt.imshow(img_target)\n",
    "plt.figure('optimization_result')\n",
    "plt.imshow(np.asarray(res[0].detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body.childs[1].childs[0].head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

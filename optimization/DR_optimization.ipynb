{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization by DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/ik/Desktop/zepeto')\n",
    "from a_python.rigging_class.rig_hier_maya_torch import *\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import pytorch3d\n",
    "from a_python.utils.rot import _rot_base\n",
    "\n",
    "from pytorch3d.io import load_objs_as_meshes, save_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVOrthographicCameras,\n",
    "    OpenGLOrthographicCameras,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    SoftPhongShader\n",
    ")\n",
    "from pytorch3d.renderer.mesh import rasterize_meshes\n",
    "from pytorch3d.renderer.mesh.rasterize_meshes import rasterize_meshes_python\n",
    "\n",
    "from pytorch3d.renderer.blending import BlendParams\n",
    "from pytorch3d.renderer.blending import sigmoid_alpha_blend, _sigmoid_alpha\n",
    "\n",
    "\n",
    "\n",
    "def img_upperbody_maya_joints(keypts):\n",
    "    '''maya관절 길이비율대로 상체 조인트 추가 : chest upper, chest, spine'''\n",
    "    img_hip = (keypts[9,:2] + keypts[12,:2])/2\n",
    "    img_neck = (keypts[2,:2] + keypts[5,:2])/2\n",
    "    img_spine = (img_neck - img_hip) * len_joint_upper[-1] + img_hip\n",
    "    img_chest = (img_neck - img_hip) * (len_joint_upper[-1] + len_joint_upper[-2]) + img_hip\n",
    "    img_chsetupper = (img_neck - img_hip) * (len_joint_upper[-1] + len_joint_upper[-2] + len_joint_upper[-3])  + img_hip\n",
    "    return img_hip, img_neck, img_spine, img_chest, img_chsetupper\n",
    "\n",
    "def fitting_based_on_upperbody_joints(body, img_spine, img_hip, img_neck, _vertices):\n",
    "    '''img_upperbody_maya_joints 함수 결과 사용해서 메쉬 스케일링으로 상체 조인트위치 맞춤'''\n",
    "    # hip to spine 거리\n",
    "    _len_spine_maya = np.linalg.norm(body.head - body.childs[0].head) \n",
    "    _len_spine_img = np.linalg.norm(img_spine - img_hip)\n",
    "\n",
    "    # 상체 조인트 맞춤\n",
    "    _spine_trans = -(_len_spine_maya - _len_spine_img) * \\\n",
    "        ((-body.head + body.childs[0].head) / np.linalg.norm(body.head - body.childs[0].head))\n",
    "    body.childs[0].trans_head(_spine_trans, _vertices)\n",
    "    _scale_spine_hier = (np.linalg.norm(img_hip - img_neck) * len_joint_upper[-1]) / \\\n",
    "        (np.linalg.norm(body.childs[0].head - body.childs[0].childs[0].head))\n",
    "    body.childs[0].scale_iso_hier(_scale_spine_hier, _vertices)\n",
    "\n",
    "def _rot_base(axis, angle):\n",
    "    zero = torch.zeros_like(angle)\n",
    "    one = torch.ones_like(angle)\n",
    "    if axis == 'x':\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                torch.stack([one, zero, zero]),\n",
    "                torch.stack([0, torch.cos(angle), -torch.sin(angle)]),\n",
    "                torch.stack([0, torch.sin(angle), torch.cos(angle)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if axis == 'y':\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                torch.stack([torch.cos(angle), zero, torch.sin(angle)]),\n",
    "                torch.stack([zero, one, zero]),\n",
    "                torch.stack([-torch.sin(angle), zero, torch.cos(angle)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if axis == 'z':\n",
    "        return torch.stack(\n",
    "            [\n",
    "                torch.stack([torch.cos(angle), -torch.sin(angle), zero]),\n",
    "                torch.stack([torch.sin(angle), torch.cos(angle), zero]),\n",
    "                torch.stack([zero, zero, one]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "def deform_mesh(opt_x, body, _vertices_opt):\n",
    "    body.childs[1].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "    \n",
    "    body.childs[1].childs[1].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "\n",
    "    body.childs[1].childs[0].rot(_rot_base('z', opt_x[0]), _vertices_opt, inplace=False)\n",
    "    body.childs[1].childs[1].rot(_rot_base('z', opt_x[1]), _vertices_opt, inplace=False)\n",
    "\n",
    "def render_changed_mesh_subimg(body, mesh, _vertices, opt_x, img_size, rasterize_size, crotch_subimg):\n",
    "    # mesh change\n",
    "    _vertices_opt = copy.deepcopy(_vertices)\n",
    "    body_copy = copy.deepcopy(body)\n",
    "    deform_mesh(opt_x, body_copy, _vertices_opt)\n",
    "\n",
    "    move = torch.tensor([rasterize_size/2, rasterize_size/2, 0]) - crotch_subimg\n",
    "\n",
    "    # 허벅지 crop에 맞게 cropped rasterize\n",
    "    mesh._verts_packed = (\n",
    "        _vertices_opt - _vertices_opt[3106,:]\\\n",
    "        + torch.tensor([0, 0, img_size/2]) + move\n",
    "        ) / (rasterize_size / 2)\n",
    "    \n",
    "    fragments = rasterize_meshes(mesh, image_size = rasterize_size, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "    blend_params = BlendParams()\n",
    "    res = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "    return res\n",
    "\n",
    "def loss_dr(img_target, opt_x, body, _vertices, crotch, crotch_subimg, img_size,rasterize_size, mesh, img_weight = None):\n",
    "    res = render_changed_mesh_subimg(body, mesh, _vertices, opt_x, img_size, rasterize_size, crotch_subimg)\n",
    "\n",
    "    # 넘치는거 막기 위한 가중치 있을 떄\n",
    "    if img_weight is None:\n",
    "        loss = (torch.abs(img_target - res)).sum()\n",
    "    else:\n",
    "        loss = (img_weight * torch.abs(img_target - res)).sum()\n",
    "    \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사진 index\n",
    "\n",
    "idx_img = 11\n",
    "idx_img = 8\n",
    "\n",
    "# optimize parameters\n",
    "n_iter = 50\n",
    "alpha_loss_excede_mask = 10          # 마스크 넘어가는거에 얼마나 가중치 줄건지\n",
    "\n",
    "visualize_pcd = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "folder = idx_img\n",
    "path_img = \"/Users/ik/Downloads/test_set_new/{}/test_resized.png\".format(folder)\n",
    "path_json = \"/Users/ik/Downloads/test_set_new/{}/test_resized_keypoints.json\".format(folder)\n",
    "path_dp = \"/Users/ik/Downloads/test_set_new/{}/dp_dump.pkl\".format(folder)\n",
    "\n",
    "with open(path_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "with open(path_dp, 'rb') as f:\n",
    "    [img_seg, img_v, img_u,_] = pkl.load(f)\n",
    "h,w = img_seg.shape\n",
    "keypts = torch.tensor(data['people'][0]['pose_keypoints_2d']).reshape([-1,3])\n",
    "img = cv.imread(path_img)\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "body = rig_class(65)\n",
    "\n",
    "\n",
    "h, w = img_seg.shape\n",
    "x = np.linspace(0, w-1, w)\n",
    "y = np.linspace(0, h-1, h)\n",
    "xx, yy = np.meshgrid(x,y, indexing='xy')\n",
    "x = torch.linspace(0, w-1, w)\n",
    "y = torch.linspace(0, h-1, h)\n",
    "\n",
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "_vertices = copy.deepcopy(vertices)\n",
    "# 가랑이 찾고, 하체 다리길이 맞추기용 키포인트 길이 구하기 : 가랑이부분 허벅지 두께, 무릎, 발목 폭\n",
    "# image랑 _vertices의 스케일이 같아짐 \n",
    "crotch = body.scaling_joint_len(_vertices, keypts, img_seg)\n",
    "len_maya_thigh_crotch, len_maya_knee, len_maya_ankle = body.len_leg_seg(_vertices)\n",
    "\n",
    "# 길이비율로 상체 오픈포즈에 마야 조인트 추가, 메쉬 맞추기\n",
    "img_hip, img_neck, img_spine, img_chest, img_chsetupper = img_upperbody_maya_joints(keypts)\n",
    "fitting_based_on_upperbody_joints(body, img_spine, img_hip, img_neck, _vertices)\n",
    "\n",
    "# base mesh for face index\n",
    "mesh = load_objs_as_meshes(['/Users/ik/Desktop/zepeto/blender/wip_find_weight.obj'], device)\n",
    "\n",
    "img_size = h\n",
    "\n",
    "# 사이즈 잘 맞춰줬나 확인\n",
    "if True:\n",
    "    # rasterization의 경우 normalize 해줘야함 [-1,1]\n",
    "    mesh._verts_list[0] = (\n",
    "        _vertices - _vertices[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) \\\n",
    "        - torch.tensor([img_size/2, img_size/2, -img_size/2])\n",
    "        )/img_size * 2\n",
    "\n",
    "    tmp_scale = torch.tensor([1.0])\n",
    "    tmp_scale.requires_grad = True\n",
    "    mesh._verts_list[0] *= tmp_scale\n",
    "    \n",
    "    pre =time.time()\n",
    "\n",
    "    fragments = rasterize_meshes(mesh, image_size = img_size, blur_radius=0, faces_per_pixel=8,\\\n",
    "        perspective_correct= False, )\n",
    "\n",
    "    print(time.time() - pre)\n",
    "\n",
    "    plt.figure('check size aligning : img_seg')\n",
    "    plt.imshow(img_seg)\n",
    "    plt.scatter(crotch[0], crotch[1])\n",
    "    plt.figure('check size aligning : rasterize with only length matching')\n",
    "    plt.imshow(np.asarray(torch.sum(fragments[1][0], dim=2).detach()/8+1))\n",
    "\n",
    "\n",
    "# body class 카피 - optimization에서 사용\n",
    "body_init = copy.deepcopy(body)\n",
    "\n",
    "img_mask = (img_seg!=0).astype(int)\n",
    "\n",
    "# 허벅지 부분 이미지\n",
    "thr_surplus = 3     # bouning box 여유분\n",
    "mask_leg = (img_seg == 10) + (img_seg == 9)\n",
    "bbox = np.array([[xx[mask_leg].min()-thr_surplus , yy[mask_leg].min()-thr_surplus],[xx[mask_leg].max()+thr_surplus , yy[mask_leg].max()+thr_surplus]]).astype(int)\n",
    "_ul, _lr = torch.max(torch.abs(torch.from_numpy(bbox) - crotch), dim=1).values\n",
    "\n",
    "rasterize_size = _ul+_lr\n",
    "\n",
    "bbox = np.array([[crotch[0] - _ul, crotch[1] - _ul],[crotch[0] + _lr, crotch[1] + _lr]]).astype(int)\n",
    "crotch_subimg = torch.tensor([_ul, _ul, 0])\n",
    "\n",
    "\n",
    "# initial rasterization and subimage aligning\n",
    "_vertices_opt = copy.deepcopy(_vertices)\n",
    "## 허벅지 crop에 맞게 cropped rasterize\n",
    "mesh_crotch = (\n",
    "    _vertices_opt[3106,:] - _vertices_opt[3106,:] \\\n",
    "    - torch.tensor([crotch[0], crotch[1], 0]) \\\n",
    "    + torch.tensor([img_size/2, img_size/2, img_size/2])\n",
    "    )/img_size * 2\n",
    "mesh_crotch = mesh_crotch.detach()\n",
    "\n",
    "# _mesh_scale = img_size / rasterize_size\n",
    "# _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] \n",
    "# _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1]\n",
    "\n",
    "move = torch.tensor([rasterize_size/2, rasterize_size/2, 0]) - crotch_subimg\n",
    "\n",
    "\n",
    "\n",
    "mesh._verts_packed = (\n",
    "    _vertices_opt - _vertices_opt[3106,:]\\\n",
    "    + torch.tensor([0, 0, img_size/2]) + move\n",
    "    ) / (rasterize_size / 2)\n",
    "\n",
    "fragments = rasterize_meshes(mesh, image_size = _ul + _lr, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "\n",
    "\n",
    "if True:\n",
    "    plt.figure('subimg')\n",
    "    plt.imshow(img_seg[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]])\n",
    "    plt.scatter(crotch_subimg[0],crotch_subimg[1])\n",
    "\n",
    "    plt.figure('rasterize_base')\n",
    "    plt.imshow(np.asarray(torch.sum(fragments[1][0], dim=2).detach()/8+1))\n",
    "    plt.scatter(crotch_subimg[0],crotch_subimg[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x = torch.nn.Parameter(torch.tensor([0.,0.,1.,1.,1.,1., 0., 0.]))\n",
    "'''opt_x : uppperlegtrans_x, uppperlegtrans_y, upperlegscale, uppertwistscale, scale_iso_upper, scale_iso_upper_twist, move_x, move_y'''\n",
    "\n",
    "## rasterize 관련\n",
    "colors = torch.ones_like(fragments[2])\n",
    "## GT : 허벅지 부분 이미지\n",
    "img_mask = (img_seg!=0).astype(int)\n",
    "thr_surplus = 3\n",
    "mask_leg = (img_seg == 10) + (img_seg == 9)\n",
    "bbox = np.array([[xx[mask_leg].min()-thr_surplus , yy[mask_leg].min()-thr_surplus],[xx[mask_leg].max()+thr_surplus , yy[mask_leg].max()+thr_surplus]]).astype(int)\n",
    "_ul, _lr = torch.max(torch.abs(torch.from_numpy(bbox) - crotch), dim=1).values\n",
    "bbox = np.array([[crotch[0] - _ul, crotch[1] - _ul],[crotch[0] + _lr, crotch[1] + _lr]]).astype(int)\n",
    "crotch_subimg = torch.tensor([_ul, _ul, 0])\n",
    "img_target = torch.from_numpy((img_seg[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]]>0).astype(int))\n",
    "tmp_img_weight = (img_seg[bbox[0,1]:bbox[1,1], bbox[0,0]:bbox[1,0]]>0).astype(int)\n",
    "img_weight = torch.from_numpy((tmp_img_weight - 1) * (-alpha_loss_excede_mask) + tmp_img_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## opitmize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "opt_x = torch.nn.Parameter(torch.tensor([0.,0.,1.,1.,1.,1., 0., 0.]))\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params' : opt_x}], lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam([{'params' : opt_x}], lr=0.01)\n",
    "\n",
    "\n",
    "loss_all = []\n",
    "result_opt_x = []\n",
    "\n",
    "\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_dr(img_target, opt_x, body, _vertices, crotch, crotch_subimg, img_size,rasterize_size, mesh, img_weight)\n",
    "    # loss = loss_dr(img_target, opt_x, body_init, _vertices, crotch, img_size, _ul, _lr, mesh, mesh_crotch, img_weight)\n",
    "    print('loss:', loss)\n",
    "    loss_all.append(loss.detach())\n",
    "    # print('opt_x:', opt_x)\n",
    "    # _result_opt_x = copy.deepcopy(opt_x)\n",
    "    # result_opt_x.append(np.asarray(_result_opt_x.detach()))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "print(\"time_all : \", time.time() - time_start)\n",
    "print(\"opt_x: \", opt_x)\n",
    "# opt_x = torch.nn.Parameter(torch.tensor([0.,0.,1.,1.,1.,1., 0., 0.]))\n",
    "res = render_changed_mesh_subimg(body, mesh, _vertices, opt_x, img_size, rasterize_size, crotch_subimg)\n",
    "\n",
    "plt.figure('GT')\n",
    "plt.imshow(img_target)\n",
    "plt.figure('optimization_result')\n",
    "plt.imshow(np.asarray(res[0].detach()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_pcd:\n",
    "    _list_joint = []\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    _vertices_opt = copy.deepcopy(_vertices)\n",
    "    body_copy = copy.deepcopy(body_init)\n",
    "    deform_mesh(opt_x, body_copy, _vertices_opt)\n",
    "\n",
    "\n",
    "    _vertices_draw = np.asarray(copy.deepcopy(_vertices_opt.detach()))\n",
    "\n",
    "    # _vertices_draw = np.asarray(copy.deepcopy(_vertices.detach()))\n",
    "    pcd.points = o3d.utility.Vector3dVector(_vertices_draw)\n",
    "    _color = np.ones([9067,3])*np.array([0.5,0.3,1])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(_color)\n",
    "    # _pcd = o3d.geometry.PointCloud()\n",
    "    # _pcd.points = o3d.utility.Vector3dVector(vertices)\n",
    "    # _list_joint.append(_pcd)\n",
    "    _list_joint.append(pcd)\n",
    "    body.draw_joint(_list_joint)\n",
    "    o3d.visualization.draw_geometries(_list_joint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/ik/Desktop/zepeto')\n",
    "from a_python.rigging_class.rig_hier_maya_torch import *\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import pytorch3d\n",
    "from a_python.utils.rot import _rot_base\n",
    "from pytorch3d.io import load_objs_as_meshes, save_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVOrthographicCameras,\n",
    "    OpenGLOrthographicCameras,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    SoftPhongShader\n",
    ")\n",
    "from pytorch3d.renderer.mesh import rasterize_meshes\n",
    "from pytorch3d.renderer.mesh.rasterize_meshes import rasterize_meshes_python\n",
    "\n",
    "from pytorch3d.renderer.blending import BlendParams\n",
    "from pytorch3d.renderer.blending import sigmoid_alpha_blend, _sigmoid_alpha\n",
    "import copy\n",
    "\n",
    "%matplotlib widget\n",
    "body = rig_class(65)\n",
    "\n",
    "_vertices = copy.deepcopy(vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DR optimize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperbody_rigging_for_joint_matching_only_body(body,_vertices, opt_upper_scale, opt_upper_rot):\n",
    "    '''상체 joint_matching용 rigging'''\n",
    "    body_init = copy.deepcopy(body)\n",
    "    _vertices_opt = copy.deepcopy(_vertices)\n",
    "\n",
    "    # spine\n",
    "    body_init.childs[0].scale_iso_hier(opt_upper_scale[0], _vertices_opt)\n",
    "    body_init.childs[0].rot(_rot_base('z', opt_upper_rot[0]), _vertices_opt, inplace=False)\n",
    "    # chestupper\n",
    "    body_init.childs[0].childs[0].childs[0].rot(_rot_base('z', opt_upper_rot[1]), _vertices_opt, inplace=False)\n",
    "    # shoulder\n",
    "    body_init.childs[0].childs[0].childs[0].childs[1].scale_iso_hier(opt_upper_scale[1], _vertices_opt)\n",
    "    body_init.childs[0].childs[0].childs[0].childs[2].scale_iso_hier(opt_upper_scale[1], _vertices_opt)\n",
    "\n",
    "    # joint\n",
    "    joint = []\n",
    "    # Left\n",
    "    ## upperarm\n",
    "    joint.append(body_init.childs[0].childs[0].childs[0].childs[1].childs[0].T_mat[:3,3][None,:])\n",
    "    # Right\n",
    "    ## upperarm\n",
    "    joint.append(body_init.childs[0].childs[0].childs[0].childs[2].childs[0].T_mat[:3,3][None,:])\n",
    "    \n",
    "    joint = torch.cat(joint, dim=0)\n",
    "    joint_aligned = joint * torch.tensor([1,-1,1]) - _vertices_opt[3106,:]* torch.tensor([1,-1,1]) + torch.tensor([crotch[0], crotch[1], 0])\n",
    "    \n",
    "    return body_init, _vertices_opt, joint_aligned\n",
    "\n",
    "def loss_dr_upper_only_body(body, _vertices, opt_upper_scale, opt_upper_rot, gt_joint):\n",
    "    body_init, _vertices_opt, joint = upperbody_rigging_for_joint_matching_only_body(body, _vertices, opt_upper_scale, opt_upper_rot)\n",
    "    loss = torch.abs(gt_joint[:,:2] - joint[:,:2]).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.8997e-01,  7.2002e-01,  7.4207e-02, -7.1548e+00],\n",
      "        [ 7.1665e-01, -6.9393e-01,  6.9782e-02,  7.3362e+01],\n",
      "        [ 1.0174e-01,  5.0328e-03, -9.9480e-01, -1.3653e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([-7.1548, 73.3624, -1.3653], grad_fn=<SelectBackward0>)\n",
      "tensor([0.6900, 0.7166, 0.1017], grad_fn=<DivBackward0>)\n",
      "tensor([ 0.7200, -0.6939,  0.0050], grad_fn=<DivBackward0>)\n",
      "tensor([ 0.0742,  0.0698, -0.9948], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "body = rig_class(65)\n",
    "body_time = copy.deepcopy(body)\n",
    "_vertices_time = copy.deepcopy(vertices)\n",
    "\n",
    "\n",
    "def time_check(body_init, _vertices_opt):\n",
    "    # spine, shoulder\n",
    "    opt_upper_scale = torch.nn.Parameter(torch.tensor([1., 1.]))\n",
    "    # spine_z, chestuppper_z\n",
    "    opt_upper_rot = torch.nn.Parameter(torch.tensor([1., 1.])) \n",
    "    # spine, chest, chestupper scale\n",
    "    opt_x = torch.nn.Parameter(torch.tensor([1., 1., 1.])) \n",
    "    # optimize paramters\n",
    "    body_init.childs[0].scale_y(opt_x[0], _vertices_opt)\n",
    "    body_init.childs[0].childs[0].scale_y(opt_x[1], _vertices_opt)\n",
    "    body_init.childs[0].childs[0].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "\n",
    "\n",
    "    # Prior result\n",
    "    ## spine\n",
    "    t1 = time.time()\n",
    "    body_init.childs[0].scale_iso_hier(opt_upper_scale[0], _vertices_opt)\n",
    "    t2 = time.time()\n",
    "    body_init.childs[1].scale_iso_hier(opt_upper_scale[0], _vertices_opt)\n",
    "    # body_init.childs[0].rot(_rot_base('z', opt_upper_rot[0]), _vertices_opt, inplace=False)\n",
    "    ## chestupper\n",
    "    t3 = time.time()\n",
    "    body_init.childs[0].childs[0].childs[0].rot(_rot_base('z', opt_upper_rot[1]), _vertices_opt, inplace=False)\n",
    "    t4 = time.time()\n",
    "    ## shoulder\n",
    "    body_init.childs[0].childs[0].childs[0].childs[1].scale_iso_hier(opt_upper_scale[1], _vertices_opt)\n",
    "    body_init.childs[0].childs[0].childs[0].childs[2].scale_iso_hier(opt_upper_scale[1], _vertices_opt)\n",
    "\n",
    "    # print(\"time 1:\", t2-t1)\n",
    "    # print(\"time 2:\", t3-t2)\n",
    "    # print(\"time 3:\", t4-t3)\n",
    "\n",
    "\n",
    "def time_check_no_grad(body_init, _vertices_opt):\n",
    "    with torch.no_grad():\n",
    "        # spine, shoulder\n",
    "        opt_upper_scale = torch.nn.Parameter(torch.tensor([1., 1.]))\n",
    "        # spine_z, chestuppper_z\n",
    "        opt_upper_rot = torch.nn.Parameter(torch.tensor([1., 1.])) \n",
    "        # spine, chest, chestupper scale\n",
    "        opt_x = torch.nn.Parameter(torch.tensor([1., 1., 1.])) \n",
    "        # optimize paramters\n",
    "        body_init.childs[0].scale_y(opt_x[0], _vertices_opt)\n",
    "        body_init.childs[0].childs[0].scale_y(opt_x[1], _vertices_opt)\n",
    "        body_init.childs[0].childs[0].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "        \n",
    "\n",
    "        # Prior result\n",
    "        ## spine\n",
    "        t1 = time.time()\n",
    "        body_init.childs[0].scale_iso_hier(opt_upper_scale[0], _vertices_opt)\n",
    "        t2 = time.time()\n",
    "        body_init.childs[1].scale_iso_hier(opt_upper_scale[0], _vertices_opt)\n",
    "        # body_init.childs[0].rot(_rot_base('z', opt_upper_rot[0]), _vertices_opt, inplace=False)\n",
    "        ## chestupper\n",
    "        t3 = time.time()\n",
    "        body_init.childs[0].childs[0].childs[0].rot(_rot_base('z', opt_upper_rot[1]), _vertices_opt, inplace=False)\n",
    "        t4 = time.time()\n",
    "        ## shoulder\n",
    "        body_init.childs[0].childs[0].childs[0].childs[1].scale_iso_hier(opt_upper_scale[1], _vertices_opt)\n",
    "        body_init.childs[0].childs[0].childs[0].childs[2].scale_iso_hier(opt_upper_scale[1], _vertices_opt)\n",
    "    print(\"time total:\", t4-t1)\n",
    "\n",
    "body_time = copy.deepcopy(body)\n",
    "_vertices_time = copy.deepcopy(vertices)\n",
    "time_check(body_time, _vertices_time)\n",
    "\n",
    "tmp = body_time.childs[0].childs[0].childs[0].childs[2]\n",
    "print(tmp.T_mat )\n",
    "print(tmp.head)\n",
    "print(tmp.dir_longi)\n",
    "print(tmp.dir_y)\n",
    "print(tmp.dir_z )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시간 늘어나기 전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time total: 0.009732246398925781\n"
     ]
    }
   ],
   "source": [
    "body_time = copy.deepcopy(body)\n",
    "_vertices_time = copy.deepcopy(vertices)\n",
    "time_check(body_time, _vertices_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시간 늘어난 후"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어디가 오래걸리나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_upperbody_maya_joints(keypts):\n",
    "    '''maya관절 길이비율대로 상체 조인트 추가 : chest upper, chest, spine'''\n",
    "    img_hip = (keypts[9,:2] + keypts[12,:2])/2\n",
    "    img_neck = (keypts[2,:2] + keypts[5,:2])/2\n",
    "    img_spine = (img_neck - img_hip) * len_joint_upper[-1] + img_hip\n",
    "    img_chest = (img_neck - img_hip) * (len_joint_upper[-1] + len_joint_upper[-2]) + img_hip\n",
    "    img_chsetupper = (img_neck - img_hip) * (len_joint_upper[-1] + len_joint_upper[-2] + len_joint_upper[-3])  + img_hip\n",
    "    return img_hip, img_neck, img_spine, img_chest, img_chsetupper\n",
    "\n",
    "def fitting_based_on_upperbody_joints(body, img_spine, img_hip, img_neck, _vertices):\n",
    "    '''img_upperbody_maya_joints 함수 결과 사용해서 메쉬 스케일링으로 상체 조인트위치 맞춤'''\n",
    "    # hip to spine 거리\n",
    "    _len_spine_maya = np.linalg.norm(body.head - body.childs[0].head) \n",
    "    _len_spine_img = np.linalg.norm(img_spine - img_hip)\n",
    "\n",
    "    # 상체 조인트 맞춤\n",
    "    _spine_trans = -(_len_spine_maya - _len_spine_img) * \\\n",
    "        ((-body.head + body.childs[0].head) / np.linalg.norm(body.head - body.childs[0].head))\n",
    "    body.childs[0].trans_head(_spine_trans, _vertices)\n",
    "    _scale_spine_hier = (np.linalg.norm(img_hip - img_neck) * len_joint_upper[-1]) / \\\n",
    "        (np.linalg.norm(body.childs[0].head - body.childs[0].childs[0].head))\n",
    "    body.childs[0].scale_iso_hier(_scale_spine_hier, _vertices)\n",
    "\n",
    "def _rot_base(axis, angle):\n",
    "    zero = torch.zeros_like(angle)\n",
    "    one = torch.ones_like(angle)\n",
    "    if axis == 'x':\n",
    "        return torch.stack(\n",
    "            [\n",
    "                torch.stack([one, zero, zero]),\n",
    "                torch.stack([zero, torch.cos(angle), -torch.sin(angle)]),\n",
    "                torch.stack([zero, torch.sin(angle), torch.cos(angle)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if axis == 'y':\n",
    "        return torch.stack(\n",
    "            [\n",
    "                torch.stack([torch.cos(angle), zero, torch.sin(angle)]),\n",
    "                torch.stack([zero, one, zero]),\n",
    "                torch.stack([-torch.sin(angle), zero, torch.cos(angle)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if axis == 'z':\n",
    "        return torch.stack(\n",
    "            [\n",
    "                torch.stack([torch.cos(angle), -torch.sin(angle), zero]),\n",
    "                torch.stack([torch.sin(angle), torch.cos(angle), zero]),\n",
    "                torch.stack([zero, zero, one]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "def deform_mesh(opt_x, body, _vertices_opt):\n",
    "    body.childs[1].childs[0].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[0].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "    \n",
    "    body.childs[1].childs[1].scale_y(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].scale_z(opt_x[2], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_y(opt_x[3], _vertices_opt)\n",
    "    body.childs[1].childs[1].childs[0].scale_z(opt_x[3], _vertices_opt)\n",
    "\n",
    "    body.childs[1].childs[0].rot(_rot_base('z', opt_x[0]), _vertices_opt, inplace=False)\n",
    "    body.childs[1].childs[1].rot(_rot_base('z', opt_x[1]), _vertices_opt, inplace=False)\n",
    "\n",
    "def render_changed_mesh_subimg(body, mesh, _vertices, opt_x, img_size, rasterize_size, crotch_subimg):\n",
    "    # mesh change\n",
    "    _vertices_opt = copy.deepcopy(_vertices)\n",
    "    body_copy = copy.deepcopy(body)\n",
    "    deform_mesh(opt_x, body_copy, _vertices_opt)\n",
    "\n",
    "    move = torch.tensor([rasterize_size/2, rasterize_size/2, 0]) - crotch_subimg\n",
    "\n",
    "    # 허벅지 crop에 맞게 cropped rasterize\n",
    "    mesh._verts_packed = (\n",
    "        _vertices_opt - _vertices_opt[3106,:]\\\n",
    "        + torch.tensor([0, 0, img_size/2]) + move\n",
    "        ) / (rasterize_size / 2)\n",
    "    \n",
    "    fragments = rasterize_meshes(mesh, image_size = rasterize_size, blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "    blend_params = BlendParams()\n",
    "    res = _sigmoid_alpha(fragments[3], fragments[0], blend_params.sigma)\n",
    "    return res\n",
    "\n",
    "def loss_dr(img_target, opt_x, body, _vertices, crotch, crotch_subimg, img_size,rasterize_size, mesh, img_weight = None):\n",
    "    res = render_changed_mesh_subimg(body, mesh, _vertices, opt_x, img_size, rasterize_size, crotch_subimg)\n",
    "\n",
    "    # 넘치는거 막기 위한 가중치 있을 떄\n",
    "    if img_weight is None:\n",
    "        loss = (torch.abs(img_target - res)).sum()\n",
    "    else:\n",
    "        loss = (img_weight * torch.abs(img_target - res)).sum()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def visualize_vertices_torch(_vertices_opt, body_init, __color=np.array([0.5,0.3,1]), _chosen_idx=None):\n",
    "    _list_joint = []\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    _vertices_draw = np.asarray(copy.deepcopy(_vertices_opt.detach()))\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(_vertices_draw)\n",
    "    _color = np.ones([9067,3])*__color\n",
    "    \n",
    "    if _chosen_idx is not None:\n",
    "        _color[_chosen_idx,:] = np.array([1, 0, 0])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(_color)\n",
    "    \n",
    "    _list_joint.append(pcd)\n",
    "    body_init.draw_joint(_list_joint)\n",
    "    o3d.visualization.draw_geometries(_list_joint)\n",
    "\n",
    "\n",
    "# def visualize_vertices_torch_from_param(_vertices, body_init, opt_upper_scale, opt_upper_rot, __color=np.array([0.5,0.3,1])):\n",
    "#     _list_joint = []\n",
    "#     pcd = o3d.geometry.PointCloud()\n",
    "#     body_init, _vertices_opt,_ = upperbody_rigging_for_joint_matching(body,_vertices, opt_upper_scale, opt_upper_rot)\n",
    "#     _vertices_draw = np.asarray(copy.deepcopy(_vertices_opt.detach()))\n",
    "\n",
    "#     pcd.points = o3d.utility.Vector3dVector(_vertices_draw)\n",
    "#     _color = np.ones([9067,3])*__color\n",
    "#     pcd.colors = o3d.utility.Vector3dVector(_color)\n",
    "\n",
    "#     _list_joint.append(pcd)\n",
    "#     body_init.draw_joint(_list_joint)\n",
    "#     o3d.visualization.draw_geometries(_list_joint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scales: tensor(2.5097) , tensor(0.8982) , tensor(0.9761) , tensor(0.8403)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pytorch3d/io/obj_io.py:542: UserWarning: No mtl file provided\n",
      "  warnings.warn(\"No mtl file provided\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time total: 0.008342981338500977\n"
     ]
    }
   ],
   "source": [
    "# 사진 index\n",
    "\n",
    "idx_img = 11\n",
    "idx_img = 8\n",
    "\n",
    "# optimize parameters\n",
    "n_iter = 50\n",
    "alpha_loss_excede_mask = 10          # 마스크 넘어가는거에 얼마나 가중치 줄건지\n",
    "\n",
    "visualize_pcd = False\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "time_start = time.time()\n",
    "folder = idx_img\n",
    "path_img = \"/Users/ik/Downloads/test_set_new/{}/test_resized.png\".format(folder)\n",
    "path_json = \"/Users/ik/Downloads/test_set_new/{}/test_resized_keypoints.json\".format(folder)\n",
    "path_dp = \"/Users/ik/Downloads/test_set_new/{}/dp_dump.pkl\".format(folder)\n",
    "\n",
    "with open(path_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "with open(path_dp, 'rb') as f:\n",
    "    [img_seg, img_v, img_u,_] = pkl.load(f)\n",
    "h,w = img_seg.shape\n",
    "keypts = torch.tensor(data['people'][0]['pose_keypoints_2d']).reshape([-1,3])\n",
    "img = cv.imread(path_img)\n",
    "\n",
    "\n",
    "\n",
    "body = rig_class(65)\n",
    "\n",
    "\n",
    "x = np.linspace(0, w-1, w)\n",
    "y = np.linspace(0, h-1, h)\n",
    "xx, yy = np.meshgrid(x,y, indexing='xy')\n",
    "\n",
    "\n",
    "_vertices = copy.deepcopy(vertices)\n",
    "# 가랑이 찾고, 하체 다리길이 맞추기용 키포인트 길이 구하기 : 가랑이부분 허벅지 두께, 무릎, 발목 폭\n",
    "# image랑 _vertices의 스케일이 같아짐 \n",
    "crotch = body.scaling_joint_len(_vertices, keypts, img_seg)\n",
    "\n",
    "# 길이비율로 상체 오픈포즈에 마야 조인트 추가, 메쉬 맞추기\n",
    "img_hip, img_neck, img_spine, img_chest, img_chsetupper = img_upperbody_maya_joints(keypts)\n",
    "fitting_based_on_upperbody_joints(body, img_spine, img_hip, img_neck, _vertices)\n",
    "\n",
    "# base mesh for face index\n",
    "mesh = load_objs_as_meshes(['/Users/ik/Desktop/zepeto/blender/wip_find_weight.obj'], device)\n",
    "\n",
    "img_size = h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# body class 카피 - optimization에서 사용\n",
    "body_init = copy.deepcopy(body)\n",
    "\n",
    "img_mask = (img_seg!=0).astype(int)\n",
    "\n",
    "# 허벅지 부분 이미지\n",
    "thr_surplus = 3     # bouning box 여유분\n",
    "mask_leg = (img_seg == 10) + (img_seg == 9)\n",
    "bbox = np.array([[xx[mask_leg].min()-thr_surplus , yy[mask_leg].min()-thr_surplus],[xx[mask_leg].max()+thr_surplus , yy[mask_leg].max()+thr_surplus]]).astype(int)\n",
    "_ul, _lr = torch.max(torch.abs(torch.from_numpy(bbox) - crotch), dim=1).values\n",
    "\n",
    "rasterize_size = _ul+_lr\n",
    "\n",
    "bbox = np.array([[crotch[0] - _ul, crotch[1] - _ul],[crotch[0] + _lr, crotch[1] + _lr]]).astype(int)\n",
    "crotch_subimg = torch.tensor([_ul, _ul, 0])\n",
    "\n",
    "\n",
    "# initial rasterization and subimage aligning\n",
    "_vertices_opt = copy.deepcopy(_vertices)\n",
    "## 허벅지 crop에 맞게 cropped rasterize\n",
    "mesh_crotch = (\n",
    "    _vertices_opt[3106,:] - _vertices_opt[3106,:] \\\n",
    "    - torch.tensor([crotch[0], crotch[1], 0]) \\\n",
    "    + torch.tensor([img_size/2, img_size/2, img_size/2])\n",
    "    )/img_size * 2\n",
    "mesh_crotch = mesh_crotch.detach()\n",
    "\n",
    "# _mesh_scale = img_size / rasterize_size\n",
    "# _move_x = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[0] \n",
    "# _move_y = 2*_ul/(_ul + _lr) - 1 - _mesh_scale * mesh_crotch[1]\n",
    "\n",
    "move = torch.tensor([rasterize_size/2, rasterize_size/2, 0]) - crotch_subimg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "# rasterization의 경우 normalize 해줘야함 [-1,1]\n",
    "mesh._verts_list[0] = (\n",
    "    _vertices - _vertices[3106,:] + torch.tensor([crotch[0], crotch[1], 0]) \\\n",
    "    - torch.tensor([img_size/2, img_size/2, -img_size/2])\n",
    "    )/img_size * 2\n",
    "\n",
    "tmp_scale = torch.tensor([1.0])\n",
    "tmp_scale.requires_grad = True\n",
    "mesh._verts_list[0] *= tmp_scale\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fragments = rasterize_meshes(mesh, image_size = (128, 64), blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "# fragments = rasterize_meshes(mesh, image_size = int(_ul + _lr), blur_radius=0, faces_per_pixel=8, perspective_correct= False, )\n",
    "\n",
    "\n",
    "body_time = copy.deepcopy(body)\n",
    "_vertices_time = copy.deepcopy(vertices)\n",
    "time_check(body_time, _vertices_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0: 3.0040740966796875e-05\n",
      "time 1: 0.9655778408050537\n",
      "time 2: 2.1457672119140625e-06\n",
      "time 3: 0.004171133041381836\n",
      "time 4: 0.00015878677368164062\n",
      "===============\n",
      "0.9702088832855225\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "body_time = copy.deepcopy(body)\n",
    "_vertices_time = copy.deepcopy(vertices)\n",
    "time_check(body_time, _vertices_time)\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "\n",
    "n_iter = 5\n",
    "\n",
    "# spine, shoulder\n",
    "opt_upper_scale = torch.nn.Parameter(torch.tensor([1., 1.]))\n",
    "# spine_z, chestuppper_z\n",
    "opt_upper_rot = torch.nn.Parameter(torch.tensor([0., 0.])) \n",
    "body_init, _vertices_opt, joint = upperbody_rigging_for_joint_matching_only_body(body, _vertices, opt_upper_scale, opt_upper_rot)\n",
    "\n",
    "if False:\n",
    "    joint_draw_aligned = np.asarray(joint.detach())\n",
    "\n",
    "    %matplotlib widget\n",
    "    plt.figure('from rigged')\n",
    "    plt.imshow(img_seg)\n",
    "    plt.scatter(joint_draw_aligned[:,0], joint_draw_aligned[:,1])\n",
    "\n",
    "    plt.figure('aligned')\n",
    "    plt.imshow(img_seg)\n",
    "    plt.scatter(joint_draw_aligned[:,0], joint_draw_aligned[:,1])\n",
    "    plt.scatter(keypts[1:8,0], keypts[1:8,1])\n",
    "\n",
    "gt_index = [5,2]\n",
    "gt_joint = keypts[gt_index,:]\n",
    "\n",
    "#######################################################\n",
    "body_time = copy.deepcopy(body)\n",
    "_vertices_time = copy.deepcopy(vertices)\n",
    "time_check(body_time, _vertices_time)\n",
    "#######################################################\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params' : opt_upper_scale}, {'params' : opt_upper_rot}], lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam([{'params' : opt_upper_scale}, {'params' : opt_upper_rot}], lr=0.01)\n",
    "\n",
    "opt_time = time.time()\n",
    "for i in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_dr_upper_only_body(body, _vertices, opt_upper_scale, opt_upper_rot, gt_joint)\n",
    "    if i+1 % 10 == 0:\n",
    "        print('loss:', loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print(time.time() - opt_time)\n",
    "\n",
    "#######################################################\n",
    "body_time = copy.deepcopy(body)\n",
    "_vertices_time = copy.deepcopy(vertices)\n",
    "time_check(body_time, _vertices_time)\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time total: 0.006828784942626953\n",
      "time 1: 0.20571398735046387\n",
      "time 2: 0.06356096267700195\n",
      "time 3: 0.2627401351928711\n"
     ]
    }
   ],
   "source": [
    "body_time = copy.deepcopy(body)\n",
    "_vertices_time = copy.deepcopy(vertices)\n",
    "time_check_no_grad(body_time, _vertices_time)\n",
    "time_check(body_time, _vertices_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spine_scale\n",
      "pelvis_scale\n"
     ]
    }
   ],
   "source": [
    "print(body_init.childs[0].name)\n",
    "print(body_init.childs[1].name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
